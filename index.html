<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h3>A Multiverse of Glorious Documentation</h3>
					<br>
					<span>Lucas Pluvinage, Tarides</span>,<br>
					<span>Jon Ludlam, University of Cambridge</span>
					<aside class="notes">
						Hello. My name is Jon Ludlam, and I'll be giving our talk entitled
						"A Multiverse of Glorious Documentation", written by Lucas Pluvinage and myself. Those of you that were here
						last year may remember I gave a talk about `odoc` entitled 'The Final
						Piece of the Documentation Puzzle'. Now, you might notice
						I'm back with more documentation work so you might reasonably think perhaps last's year's title
						was maybe a little premature. The difference this year however, is that I want to expand our scope.
						We weren't ambitious enough last year -- we looked only at documenting a single library. This year I want
						to document every single library I've ever seen, and all the libraries that I've never
						seen too, and even every version of all of these libraries. And I'm going to tell you how to combine all those library documents
						into not just a glorious universe of documents, but an entire multiverse! Let's go!
					</aside>
				</section>

				<section>
					<h3>Goals</h3>
					<ul>
						<li>To produce correct, cross-referenced documentation for <strong><em>all</em></strong> versions of <strong><em>all</em></strong> packages</li>
						<li>To ensure this is automated, incremental, low latency and repeatable</li>
						<li>Provide a solid basis for further innovation, e.g. links to/from source, type-aware search, etc.</li>
					</ul>
					<aside class="notes">
						As usual we start with the goals. We're looking at the opam repository, and wanting to
						produce documentation for each of the versions of each of the packages in there.
						That amounts to just short of 20,000 version as of today - a lot of packages to be sure, but not insurmountable.
						In fact, we actually end up documenting a few more than that, and I'll discuss the reasons why this is the case in a bit.

						Additionally, we'd really like this to be an automated process, so we can just let 
					    it get on with things and not worry about it - I don't particular want to schedule
						a weekly 'update the docs' slot into my Monday mornings. Even better, let's have it
						such that any time a new package or version is merged into opam-repository.git
						we'll build docs for that. While we're at it, let's have it happen quickly,
						let's aim for 10 minutes and see how we do. And if the server we're running it on
						dies a horrible death, we need to be able to start it all up again without
						too many tears.

						OK, so we know what we want. How are we going to do it?
					</aside>
				</section>

				<section>
					<h3>Odoc and HTML file placement</h3>
					<ul>
						<li>Use <a href="https://ocaml.github.io/odoc">odoc</a></li>
						<li>Odoc requires consistent set of packages compiled together</li>
						<li>Can’t have more than one version then!</li>
						<li>Solution: build docs separately then ‘union’ them</li>
						<li>Odoc 2.0 supports this</li>
					</ul>
					<aside class="notes">
						No surprises here given my talk last year -- we're going to use odoc. It's the only tool in
						the game really. The trouble is that the way odoc works is that it requires a consistent
						set of packages that are all co-installable; or more accurately co-typable. To understand
						why see my talk from last year. So that rules out having more than one version of a package
						... in most cases at least.

						What we're going to do then, is build the documents separately into some carefully defined
						file tree, then 'union' the trees together into one set of docs.

						Happily, Odoc 2.0 supports rendering HTML into an arbitrary file structure, so we're good
						to go.

						Our next decision is therefore: exactly what file layout will we use?
					</aside>
				</section>

				<section>
					<h3>Universes</h3>
					<ul>
						<li>Could we simply use <pre><code class="language-shell">/packages/&lt;package name&gt;/&lt;package version&gt;/</code></pre></li>
						<li>Doesn't specify precisely enough: A package interface depends upon the precise versions of its dependencies! Consider:</li>
						<pre><code class="language-ocaml">
module M : Stdlib.Set.S with type t = mytype
						</code></pre>
					</ul>
					<aside class="notes">
						The most obvious answer is just to stick the HTML docs into a file structure based on the package name and version.
						Is this sufficient? Sadly no it's not.
						
						The fact is that the interface exposed by a package is dependent not only on the package itself, but on the _exact versions_ of its dependencies.
						
						Consider, for example, a package that exposes a `Set` created from the functor in the standard library - specialised in this case that the type t is equal to "mytype".
						In this signature, the function 'filter_map' has only been included since OCaml 4.11, so if you build this exact package against 4.10 it would not contain that value.
						Obviously the standard library is not unique in this, so it's entirely possible for the interface of a package to change if you change the version of any of its dependencies.
						
						Consequently in order for the cross-referencing to be exact and correct, we need to identify a particular set of docs for a package not just by the package name and version,
						but by the triple of 'package name', 'package version' and 'universe id', the last of which we define as a hash dependent on the package's versioned dependencies.
					</aside>
				</section>

			    <section>
					<h3>Multiverse</h3>
					<ul>
					<li>Can we get away with a single universe per package?</li>
					<li>No! Example: `findlib`</li>
					<li>So put the universe id in the path<pre><code class="language-shell">/packages/&lt;universe id&gt;/&lt;package name&gt;/&lt;package version&gt;/</code></pre></li>

					</ul>
					<aside class="notes">

						Next question: Can we get away with only ever compiling each version of each package against one set of dependencies?
						It would be convenient, but this turns out not to be possible. Take the example of `findlib` - virtually all packages
						depend upon findlib, and with no constraint, meaning opam will pick the latest compatible version. However, many packages
						have constraints on the version of OCaml, so we end up compiling the latest version of `findlib` against many different
						versions of the compiler.

						Consequently, for some packages, we necessarily need to produce documentation for the same version for each different
						universe in which it is needed. Since these may be different, we need to ensure these are put in different paths. The
						most obvious way to do this is to use the universe id as part of the path, so something like /packages/universe id/package name/package version

					</aside>
				</section>

				<section>
					<h3>"Blessed" packages</h3>
					<ul>
					<li>These URLs are awful!</li>
					<li>Pick one universe amongst the many to be the 'standard' one, with a reasonable URL</li>
					<li>
						<pre><code>
/p/<i>package name</i>/<i>package version</i>/
/u/<i>universe id 1</i>/<i>package name</i>/<i>package version</i>/
/u/<i>universe id 2</i>/<i>package name</i>/<i>package version</i>/
						</code></pre>
					</ul>
					<li>these chosen universes are called 'blessed' universes</li>
					<aside class="notes">

					While this scheme is correct, it doesn't lead to particularly discoverable, pleasant or easy-to-remember URLs.

					To fix this, what we do is pick one particular universe to be the 'representative' for each version of each
					package. I'll describe later the exact mechanism we use for this at the moment, but the important thing is that
					this gives us a way to have a nice discoverable and easy-to-use URL for each package version, while retaining
					the 'correctness' that every link can be guaranteed to go to the destination that the package was actually
					compiled against.

					We call the chosen representative universe the 'blessed' universe.
					</aside>
				</section>

				

				<section>
					<h3>Incremental Pipeline for Fast Builds</h3>
					<img src="pipeline.png" />
					<ul>
						<li>Track the opam repository</li>
						<li>Solve for each package version</li>
						<li>Install each resolution and extract the installed files</li>
						<li>Compile the documentation for each package and extract the compiled files</li>
						<li>Generate the HTML files from the compiled files</li>
						</ul>
						<aside class="notes">
						We use ocurrent to build an incremental pipeline to build the docs. This is the
						same system that's being used for CI systems such as ocaml-ci - it's designed to
						watch git repositories for updates, and responding by submitting jobs to a cluster
						for execution. It's got some really nice caching built in that works really well,
						though not quite capable enough today to match our needs, so
						we have built more of a stateful pipeline. Not only that, but since the artefacts
						we work with are large we store them on an external filesystem, and that in itself
						is a stateful part - so we have two states to keep in sync.

						Ocurrent encouranges incremental builds so that even
						when the build graph changes -- for example, when a new package or version is 
						committed into opam-repository -- it only rebuilds the new artefacts required. It
						does this by defining a key/value pair for each step in the build process. It then memoizes the results, so
						if it has already calculated the value
						associated with the key or keys, it just returns the previously calculated result.

						Let's take a look in more detail at some of these individual parts.
						</aside>
				</section>

				<section>
					<h3>Incremental Solver</h3>
					<ul>
						<li><strong>Key:</strong> opam repository commit id, list of tracked packages</li>
						<li><strong>State:</strong> currently known resolutions</li>
						<li><strong>value:</strong> resolutions for tracked packages</li>
					</ul>
						<aside class="notes">
							The solver's job is, for each package/version we are tracking, to run an opam solver to find the precise set of packages required to install it.
							The inputs -- the 'key' -- is the commit hash of opam-repository, and the list of the packages we're interested in,
							and the resulting value is the list of solutions.

							We keep some additional 'state' -- the list of previously solved packages -- so that when a the input changes, which is most often a new commit on opam-repository,
							we can decide whether our previous solutions are still valid.

							Right now, if we have a cached version we'll always reuse it, but we'll likely adjust this in the future. There are a variety of
							circumstances under which we'll want to re-solve -- For example, we'll almost certainly
							want to re-solve for everything when a new compiler is released, as in many cases the solutions will change to use this newly released compiler. Possibly we'll also want
							to re-solve just on a regular cadence to ensure our solutions are reasonably up-to-date.

							We do not insist on installing the optional dependencies where they exist, as it's perfectly possible for optional dependencies to
							be mutually exclusive, or for some broken packages for the optional dependencies to cause the build to fail. To some extent we do
							make up for this in the blessing step.
						</aside>
				</section>

				<section>
					<h3>Preparation/Blessing</h3>
					<ul>
						<li><strong>Key:</strong> a resolution to install, the opam-repository commit id, and the commit of 'voodoo'</li>
						<li><strong>State:</strong> external filesystem with the results of 'voodoo-prep'</li>
						<li><strong>value:</strong> hash of the results of 'voodoo-prep'</li>
					</ul>
					<p>When builds complete we choose the blessed universe:</p>
					<ul>
						<li>Most dependencies - including optional dependencies</li>
						<li>tie breaker: most reverse dependencies</li>
					</ul>
					
						<aside class="notes">
							Once we've got a solution the next step is to install it. The pipeline takes care of choosing a
							base docker image to use with an appropriate compiler version, then invokes opam to install
							the packages.

							It then runs the tool `voodoo-prep` to find the files within the opam folder that are necessary 
							to produce the docs. Mostly this is the cmti, cmt and cmi files that odoc requires, but we
							also pull out a few more files -- any package docs installed, dune-package files for a bit of
							metadata, and some info from the 'cma' files if dune-package doesn't exist.

							These files are then tarred up and pushed to the external filesystem, and the hash calculated.
							This hash is the 'value' of this part of the pipeline.

							When we've got the complete builds we can choose which universe to be blessed. We do this by picking
							a universe with the most dependencies. Doing this means that when we've got a successful build with
							some of a packages optional dependencies, which occurs when a dependent package has a non-optional
							dependency, then the blessed universe will have the docs for the packages optional libraries.

							When a tie breaker is needed we simply count the number of dependent universes, and choose the universe
							that's included in more of our other universes.

						</aside>
				</section>

				<section>
					<h3>"Compilation"</h3>
					<ul>
						<li><strong>Key:</strong> 'Preparation' step hash for the package, 'Compilation' step hash for the package's dependencies, voodoo-do and opam commit ids.
</li>
						<li><strong>State:</strong> external filesystem with the results of 'voodoo-do'</li>
						<li><strong>value:</strong> hash of the output</li>
					</ul>
						<aside class="notes">
							Once we have the artefacts extracted from the opam directory, the next step is perform the odoc 'compile' and
							'link' steps. These are roughly analogous to the usual compile and link steps; to compile the docs we
							require the results of compiling all of the dependencies, hence this forms part of the 'key' of this step.
							Furthermore this is the first step in which odoc is required, and since we are using an unreleased version
							of odoc we track the commit id here too. For more details on this step, see last year's talk.

							The result of this is the 'linked' version of the docs that can then be converted to a variety of output formats.
						</aside>
				</section>

				<section>
					<h3>HTML generation</h3>
					<ul>
						<li><strong>Key:</strong> Artifacts hash of the package's linked output</li>				  
						<li><strong>State:</strong> external filesystem with the results of 'voodoo-gen'</li>
						<li><strong>value:</strong> hash of the HTML files</li>
					</ul>
						<aside class="notes">
							The separation of this step is done largely to ensure we can rerun this quickly as
							the desired output changes - to rerun this step over all universes only talks about
							90 minutes. We have a slightly altered HTML generation step than odoc usually has, which
							has allowed the integration of the docs into other
							sites easily, in particular of course, v3.ocaml.org. This HTML generation tool is currently
							being prototyped in the `voodoo` suite of tools, but it's likely we'll fold it back into
							odoc once we decide on the most useful output format.
						</aside>
				</section>

				<section>
					<h3>Atomic updates</h3>
					<ul>
						<li>Distinguish between breaking and non-breaking changes</li>
						<li>Non-breaking changes (e.g. new package added) can be done non-atomically</li>
						<li>Breaking changes need atomic updates - via symlink on server</li>
					</ul>
						<aside class="notes">
							As with all 'live' systems we need to have a policy on how updates are handled. In
							general the updates are either breaking changes or non-breaking changes. As an 
							example of a non-breaking change we might have a new package put into opam-repository -
							this could be done simply by inserting the new set of HTML files into the appropriate
							place in the 'unioned' filesystem.

							A breaking change is anything that might cause links to break, or the
							downstream consumers to break. For example, if we changed the layout of the metadata
							consumed by the v3.ocaml.org server, we might wish to do that atomically across all
							packages rather than just incrementally. For these sorts of changes we have what we
							call an 'epoch' symlink on the server. When the build has completely finished, we can 
							simply flip the symlink and we atomically switch from one set of metadata to the
							new set.
						</aside>
				</section>

				<section>
					<h3>Stats</h3>
					<ul>
						<li>50G of HTML files</li>
<li>3300 packages, totalling 17400 versions (~2K opam installation failures)</li>
<li>18600 versions living in parallel universes</li>
<li>3.3M documentation pages</li>
<li>Package having the largest number of parallel universes: logs.0.7.0 (209 universes)</li>
<li>6 hours build on the ocamllabs cluster</li>
					</ul>
				</section>
				<section>
					<h3>Further work</h3>
					<ul>
						<li>Optimizing job duration and data movement</li>
						<li>Towards a formalisation of data pipelines ?</li>
						<li>Public APIs to access the CI output and build new frontends</li>
						<li>VSCode integration</li>
						<li>Search</li>
						<li>Comparison/coalescing of different universes</li>
						<li>Diffs between versions</li>
					</ul>
						<aside class="notes">
We have plenty of work left to do! As I mentioned before we've set up the pipeline to store its artefacts externally, and this is done in a fairly
naive way, and there are plenty of opportunities to improve this. Additionally or alternatively we might invest in embedding the idea of a data
pipeline more directly in ocurrent.

We've already got a public API of sorts in place whereby the documentation generated by the pipeline can be consumbed by the v3.ocaml.org server
and rendered nicely as part of the new OCaml website. It would be interesting to think more about other uses of this; for example we might embed
the output in the visual studio code plugin in some form, or we might be able to use the data we're generating in alternate ways; for example
some form of search. 

We've not done a lot yet on comparing the documentation in different universes. While it's certainly theoretically different, it would be useful
and informative to understand how different it can be in practice.

While we're considering comparing the output, it would also be really useful to be able to show the diffs between different versions of a package,
although since they may be compiled against different dependencies we'd need to carefully consider what the source of the differences is.

In short, there remains much work to be done!


						</aside>
				</section>


			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
